search_gs = {
  #"n_estimators": [None, 150, 200],
  "criterion": ["gini", "entropy", "log_loss"],
  "class_weight": [None, "balanced"]
}

==> {'class_weight': None, 'criterion': 'log_loss'}

-----------------------------------------

search_gs = {
  "n_estimators": [100, 120],
  "criterion": ["gini", "entropy", "log_loss"],
  "class_weight": [None, "balanced"]
}

==> {'class_weight': 'balanced', 'criterion': 'log_loss', 'n_estimators': 100}

-----------------------------------------

# For GradientBoostingClassifier
search_gs = {
  "loss": ["log_loss", "exponential"],
  "n_estimators": [100, 120],
  "max_depth": [3, 5],
  "min_samples_split": [2, 3],
  "max_leaf_nodes": [None, 10],
  "min_samples_leaf": [1, 2]
}

==> {'loss': 'log_loss', 'max_depth': 5, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}

-----------------------------------------

# For GradientBoostingClassifier
search_gs = {
  "learning_rate": [0.1, 0.2],
  "n_estimators": [100, 150],
  "max_depth": [None, 5],
}

==> {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}

-----------------------------------------

# For GradientBoostingClassifier
search_gs = {
  "max_depth": [5, 7, 9],
}

==> {'max_depth': 5}

-----------------------------------------

# For M:LPClassifier
search_gs = {
    "solver": ["adam", "sgd", "lbfgs"],
    "hidden_layer_sizes": [(100,), (200,)],
    "max_iter": [200, 300, 500],
}

==> {'hidden_layer_sizes': (200,), 'max_iter': 200, 'solver': 'sgd'}

-----------------------------------------

# For M:LPClassifier
search_gs = {
    #"solver": ["adam", "sgd", "lbfgs"],
    "solver": ["adam", "sgd"],
    "hidden_layer_sizes": [(200,), (300,), (400,)],
    #"max_iter": [200, 300, 500],
}

==> {'hidden_layer_sizes': (200,), 'solver': 'sgd'}

-----------------------------------------

# For lgb.LGBMClassifier

lgbParams = {
    'objective': 'binary',
    'num_iterations': 1000,
    'learning_rate': 0.02,
    #'early_stopping_round': 100,
    'seed': 42,
}
>>> Score: 0.8388599605388108

-> 'learning_rate': 0.01,
>>> Score: 0.8382469833108276

-> 'learning_rate': 0.1,
>>> Score: 0.825678661977298

-> 'learning_rate': 0.03,
>>> Score: 0.8392931300038415


2025 0214 Best
[AUC] tr:0.9855 va:0.8635
-------------------- Result --------------------
[[0.         0.98511117 0.86239499]
 [1.         0.98559425 0.87110994]
 [2.         0.98427923 0.86972707]
 [3.         0.98617804 0.87054793]
 [4.         0.98545149 0.86354791]]
[CV] tr:0.98530.0006, va:0.8675+-0.0037
[OOF] 0.8674

                   col            imp      imp_std
12       pressure_mean  110499.993125  2161.998597
1      batch_count_men   42697.107641   707.009454
8              pos_col   34626.514478  1419.774451
3             line_col   27602.328792  1031.515216
5            line_tray   18502.426816   806.998881
0          batch_count   17618.014626   936.874566
2                 line   16156.162818  1414.166831
16  temperature_1_mean   15954.683513  1377.394466
15   temperature_1_max   14905.131580   642.677179
13        pressure_min   14323.993973   453.224403

1位	TTS DSPLPD Kanaki, Toshiki	0.8692119826664031	2025-02-14 11:04:12.779654
2位	TML PE2 Ono, Atsuo	0.8686214738046817	2025-02-13 08:56:13.352738
3位	TEL IS Suyama, Takahiro	0.8672454937076893	2025-02-14 18:39:13.966010
4位	TFE GCE GTOC Kabutoya, Wataru	0.8638972358311928	2025-02-14 03:55:44.449441
5位	TTS DSSCTD Kanbe, Shota	0.8597735708153327	2025-02-14 08:35:37.940199



