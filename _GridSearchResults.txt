search_gs = {
  #"n_estimators": [None, 150, 200],
  "criterion": ["gini", "entropy", "log_loss"],
  "class_weight": [None, "balanced"]
}

==> {'class_weight': None, 'criterion': 'log_loss'}

-----------------------------------------

search_gs = {
  "n_estimators": [100, 120],
  "criterion": ["gini", "entropy", "log_loss"],
  "class_weight": [None, "balanced"]
}

==> {'class_weight': 'balanced', 'criterion': 'log_loss', 'n_estimators': 100}

-----------------------------------------

# For GradientBoostingClassifier
search_gs = {
  "loss": ["log_loss", "exponential"],
  "n_estimators": [100, 120],
  "max_depth": [3, 5],
  "min_samples_split": [2, 3],
  "max_leaf_nodes": [None, 10],
  "min_samples_leaf": [1, 2]
}

==> {'loss': 'log_loss', 'max_depth': 5, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}

-----------------------------------------

# For GradientBoostingClassifier
search_gs = {
  "learning_rate": [0.1, 0.2],
  "n_estimators": [100, 150],
  "max_depth": [None, 5],
}

==> {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}

-----------------------------------------

# For GradientBoostingClassifier
search_gs = {
  "max_depth": [5, 7, 9],
}

==> {'max_depth': 5}

-----------------------------------------

# For M:LPClassifier
search_gs = {
    "solver": ["adam", "sgd", "lbfgs"],
    "hidden_layer_sizes": [(100,), (200,)],
    "max_iter": [200, 300, 500],
}

==> {'hidden_layer_sizes': (200,), 'max_iter': 200, 'solver': 'sgd'}

-----------------------------------------

# For M:LPClassifier
search_gs = {
    #"solver": ["adam", "sgd", "lbfgs"],
    "solver": ["adam", "sgd"],
    "hidden_layer_sizes": [(200,), (300,), (400,)],
    #"max_iter": [200, 300, 500],
}

==> {'hidden_layer_sizes': (200,), 'solver': 'sgd'}

-----------------------------------------

# For lgb.LGBMClassifier

lgbParams = {
    'objective': 'binary',
    'num_iterations': 1000,
    'learning_rate': 0.02,
    #'early_stopping_round': 100,
    'seed': 42,
}
>>> Score: 0.8388599605388108

-> 'learning_rate': 0.01,
>>> Score: 0.8382469833108276

-> 'learning_rate': 0.1,
>>> Score: 0.825678661977298

-> 'learning_rate': 0.03,
>>> Score: 0.8392931300038415


